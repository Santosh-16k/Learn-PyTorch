{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch, matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "\n",
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ssl/.fastai/data/mnist.pkl.gz')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = datasets.download_data(MNIST_URL, ext='.gz'); path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(path, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\n",
    "n,c = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0059c57550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x_train[0]\n",
    "plt.imshow(img.view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.view(28,28).type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial python model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(784, 10)\n",
    "bias = torch.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar, ac = a.shape\n",
    "    br, bc = b.shape\n",
    "    assert ac==br\n",
    "    c=torch.zeros(ar,bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac):\n",
    "                c[i,j] += a[i,k]*b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = x_valid[:5]\n",
    "m2 = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), torch.Size([784, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.shape,m2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 826 ms, sys: 4.07 ms, total: 830 ms\n",
      "Wall time: 829 ms\n"
     ]
    }
   ],
   "source": [
    "%time t1=matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speeding up the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar, ac = a.shape\n",
    "    br, bc = b.shape\n",
    "    assert ac==br\n",
    "    c=torch.zeros(ar,bc)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            c[i,j] = (a[i,:]*b[:,j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 ms, sys: 186 µs, total: 1.63 ms\n",
      "Wall time: 1.39 ms\n"
     ]
    }
   ],
   "source": [
    "%time t1=matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tensor([10,20,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tensor([[1,2,3],\n",
    "           [4,5,6],\n",
    "           [7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11, 12, 13],\n",
       "        [24, 25, 26],\n",
       "        [37, 38, 39]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, None] + m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a,b):\n",
    "    ar, ac = a.shape\n",
    "    br, bc = b.shape\n",
    "    assert ac==br\n",
    "    c=torch.zeros(ar,bc)\n",
    "    for i in range(ar):\n",
    "        c[i] = (a[i].unsqueeze(-1)*b).sum(dim=0)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248 µs ± 33.1 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=matmul(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.59 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "8.96 µs ± 6.46 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=m1.matmul(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why mean and std should be 0 and 1 respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(), x_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max() + 1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh=50 #Hidden network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the reason to select number of hidden units as 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)/math.sqrt(m)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)/math.sqrt(nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0001), tensor(0.0355))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0145), tensor(0.1313))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.mean(), w2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1027), tensor(0.9925))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = relu(lin(x_valid, w1, b1)) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.1573), tensor(0.5497))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)*math.sqrt(2/m)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)*math.sqrt(2/nh)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6138), tensor(0.8583))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(lin(x_valid, w1, b1))\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    l3 = lin(l2, w2, b2)\n",
    "    return l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.31 ms ± 453 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = y_train.float(), y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24.3063)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients and Backward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ):\n",
    "    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_grad(inp, out):\n",
    "    inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # Forward pass\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "    loss = mse(out, targ)\n",
    "    \n",
    "    # Backward pass\n",
    "    mse_grad(out, targ)\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    relu_grad(l1, l2)\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)\n",
    "xt2 = x_train.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    # Forward pass\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    return mse(out, targ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         ...,\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446]]),\n",
       " tensor([[ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         ...,\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w12.grad, w1.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.) - 0.5\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w + self.b\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse:\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1, b1), ReLU(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 12.8 ms, total: 165 ms\n",
      "Wall time: 31.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.02 s, sys: 2.45 s, total: 7.47 s\n",
      "Wall time: 1.9 s\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.1502, -0.0757,  0.3895,  ...,  0.7737,  0.0048,  0.4834],\n",
       "         [ 1.1502, -0.0757,  0.3895,  ...,  0.7737,  0.0048,  0.4834],\n",
       "         [ 1.1502, -0.0757,  0.3895,  ...,  0.7737,  0.0048,  0.4834],\n",
       "         ...,\n",
       "         [ 1.1502, -0.0757,  0.3895,  ...,  0.7737,  0.0048,  0.4834],\n",
       "         [ 1.1502, -0.0757,  0.3895,  ...,  0.7737,  0.0048,  0.4834],\n",
       "         [ 1.1502, -0.0757,  0.3895,  ...,  0.7737,  0.0048,  0.4834]]),\n",
       " tensor([[ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         ...,\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446],\n",
       "         [ 1.0632, -0.0690,  0.3587,  ...,  0.7137,  0.0044,  0.4446]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.g, w1g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): raise Exception('Not Implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.) - 0.5    \n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = torch.einsum(\"bi,bj->ij\", inp, out.g)\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward(self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "        \n",
    "    def bwd(self, out, inp, targ): inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1, b1), ReLU(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81.2 ms, sys: 5.86 ms, total: 87 ms\n",
      "Wall time: 21.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 180 ms, sys: 54.4 ms, total: 234 ms\n",
      "Wall time: 58.3 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w, self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 101 ms, sys: 17.5 ms, total: 118 ms\n",
      "Wall time: 29.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 177 ms, sys: 53.7 ms, total: 231 ms\n",
      "Wall time: 57.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in, nh), nn.ReLU(), nn.Linear(nh, n_out)]\n",
    "        self.loss = mse\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x # self.loss(x.squeeze().float(), targ.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "%time loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec 9 begins from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Conv2d(1, nh, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_valid[:100]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0003, grad_fn=<MeanBackward0>),\n",
       " tensor(0.1156, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.weight.mean(), l1.weight.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(x): return x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(-0.0003, grad_fn=<MeanBackward0>),\n",
       "  tensor(0.1156, grad_fn=<StdBackward0>)),\n",
       " (tensor(-0.0006, grad_fn=<MeanBackward0>),\n",
       "  tensor(0.1130, grad_fn=<StdBackward0>)))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(l1.weight), stats(l1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.modules.conv??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.2420,  0.2503,  0.0211, -0.0200, -0.1312],\n",
       "          [ 0.0245,  0.0533, -0.3607, -0.1565,  0.2103],\n",
       "          [ 0.1265, -0.1416,  0.0575, -0.0163,  0.0886],\n",
       "          [-0.1485, -0.3231,  0.0939,  0.0231,  0.1901],\n",
       "          [-0.0593, -0.0679,  0.0377,  0.0311, -0.1450]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0429, -0.3494,  0.1499,  0.0929, -0.1209],\n",
       "          [ 0.1856,  0.2776, -0.0111,  0.2156, -0.3477],\n",
       "          [-0.2433, -0.3081, -0.2735,  0.1366,  0.0970],\n",
       "          [-0.4858, -0.1862, -0.1674,  0.0606, -0.0481],\n",
       "          [-0.3028,  0.1749, -0.1607, -0.2291, -0.2883]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1760,  0.0527,  0.3034,  0.1388,  0.0454],\n",
       "          [ 0.0909, -0.0626,  0.1673, -0.2723,  0.1457],\n",
       "          [-0.1469, -0.3661, -0.2363,  0.1017, -0.0346],\n",
       "          [ 0.0863, -0.0642,  0.0222, -0.1394,  0.3011],\n",
       "          [ 0.1926,  0.2031,  0.0809, -0.1867, -0.1921]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.0915, -0.0550, -0.0304, -0.5906, -0.2486],\n",
       "          [-0.0747,  0.1807, -0.0579, -0.2048, -0.0565],\n",
       "          [-0.1856, -0.2532, -0.2869, -0.1923,  0.0871],\n",
       "          [ 0.4794,  0.1668, -0.2517, -0.0569, -0.3389],\n",
       "          [ 0.1209,  0.3025, -0.2549, -0.0294, -0.5961]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0883, -0.4312, -0.1008, -0.0917, -0.2834],\n",
       "          [ 0.2765, -0.3868,  0.1581, -0.1145, -0.2392],\n",
       "          [ 0.0284, -0.1716, -0.2246, -0.1455,  0.2931],\n",
       "          [-0.1990,  0.2182, -0.0952, -0.1065,  0.1820],\n",
       "          [-0.2700,  0.1394, -0.0837,  0.2443, -0.4780]]],\n",
       "\n",
       "\n",
       "        [[[-0.1899, -0.2107, -0.1966, -0.2748, -0.0880],\n",
       "          [ 0.3449, -0.0274, -0.3756, -0.0648, -0.0250],\n",
       "          [-0.0506, -0.0856,  0.0104, -0.3072, -0.0239],\n",
       "          [ 0.2345,  0.1221,  0.1334,  0.0073, -0.2421],\n",
       "          [-0.0645,  0.0799, -0.1281,  0.0559, -0.0646]]]], requires_grad=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal_(l1.weight, a=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0052, grad_fn=<MeanBackward0>),\n",
       " tensor(0.2015, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(l1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0031, grad_fn=<MeanBackward0>),\n",
       " tensor(0.2036, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal_(l1.weight, a=1)\n",
    "stats(l1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 5, 5])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_fs = l1.weight[0,0].numel()\n",
    "rec_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf, ni, *_ = l1.weight.shape\n",
    "nf, ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 1250)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fan_in = ni*rec_fs\n",
    "fan_out = nf*rec_fs\n",
    "fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(a): return math.sqrt(2.0 / (1 + a ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 1.4142135623730951,\n",
       " 1.4141428569978354,\n",
       " 1.4071950894605838,\n",
       " 0.5773502691896257)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain(1), gain(0), gain(0.01), gain(0.1), gain(math.sqrt(5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 0., 4.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.8406, -2.2103, -2.1020], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_pred[[0,1,2], [5,0,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): return -input[range(target.shape[0]), target.long()].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nll(sm_pred, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2621, grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): return x - x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1634,  0.0692, -0.0268,  0.1276,  0.1130, -0.5341,  0.0367,  0.3491,\n",
       "         -0.1697,  0.0038], grad_fn=<SelectBackward>), torch.Size([64, 10]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = F.cross_entropy\n",
    "bs=64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2364, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "loss_func(preds, yb.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1719)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i + bs\n",
    "        \n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model(xb), yb.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, 'weight'):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4167, grad_fn=<NllLossBackward>), tensor(0.8750))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using parameters and optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in, nh)\n",
    "        self.l2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, x): return self.l2(F.relu(self.l1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(m, nh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_children of Model(\n",
       "  (l1): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (l2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs+1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb.long())\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1897, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: p -= p.grad * lr\n",
    "                \n",
    "    def zero_grad(self):\n",
    "        for p in self.params: p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs+1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb.long())\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6357, grad_fn=<NllLossBackward>), tensor(0.8125))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD.step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, 10))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3014, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "loss_func(model(xb), yb.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5018, grad_fn=<NllLossBackward>), tensor(0.8750))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs+1):\n",
    "            start_i = i*bs\n",
    "            end_i = start_i + bs\n",
    "\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            loss = loss_func(model(xb), yb.long())\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "fit()\n",
    "loss, acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert acc > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets an DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x, self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(train_ds)==len(x_train)\n",
    "assert len(valid_ds)==len(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "         [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "         [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "         [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245],\n",
       "         [-0.4245, -0.4245, -0.4245,  ..., -0.4245, -0.4245, -0.4245]]),\n",
       " tensor([5., 0., 4., 1., 9.]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_ds[0:5]\n",
    "assert xb.shape==(5,28*28)\n",
    "assert yb.shape==(5,)\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, opt = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i + bs\n",
    "\n",
    "        xb, yb = train_ds[i*bs : i*bs+bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb.long())\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2850, grad_fn=<NllLossBackward>), tensor(0.9375))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())\n",
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds, bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "assert xb.shape==(bs, 28*28)\n",
    "assert yb.shape==(bs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28, 28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4500, grad_fn=<NllLossBackward>), tensor(0.9062))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb.long())\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "fit()\n",
    "loss, acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, ds, bs, shuffle=False):\n",
    "        self.n, self.bs, self.shuffle = len(ds),bs,shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)\n",
    "        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = Dataset(*train_ds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, False)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 3, 7]), tensor([0, 6, 2]), tensor([8, 9, 4]), tensor([5])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Sampler(small_ds, 3, True)\n",
    "[o for o in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    xs, ys = zip(*b)\n",
    "    return torch.stack(xs), torch.stack(ys)\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, sampler, collate_fn=collate):\n",
    "        self.ds, self.sampler, self.collate_fn = ds, sampler, collate_fn\n",
    "    def __iter__(self):\n",
    "        for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samp = Sampler(train_ds, bs, shuffle=True)\n",
    "valid_samp = Sampler(valid_ds, bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, sampler=train_samp, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, sampler=valid_samp, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPAUlEQVR4nO3df5BV9XnH8c8jPxWNZUEQEUUIaaVaid3gDzqMrdUhTBK0ia1OaknHycZRE82kaWyaNP7RyRBN6jiZmCkaDIk/UBM1mlCU2XFqopG4UORHkUIpFWTLVqkFGsKP5ekfe8xscM/3Lvece8+F5/2a2bl3z3PPnofLfvbce7/nnK+5uwAc/06ougEAzUHYgSAIOxAEYQeCIOxAEEObubHhNsJHalQzNwmE8iv9nw74fhuoVijsZjZH0j2Shki6390XpB4/UqN0kV1eZJMAElZ4Z26t7pfxZjZE0rckfVDSdEnXmdn0en8egMYq8p59pqTN7r7F3Q9IWiJpXjltAShbkbBPlLSt3/fbs2W/wcw6zKzLzLoOan+BzQEookjYB/oQ4F3H3rr7Qndvd/f2YRpRYHMAiigS9u2SJvX7/kxJO4q1A6BRioT9FUnTzOwcMxsu6VpJT5fTFoCy1T305u6HzOwWSc+qb+htkbuvL60zAKUqNM7u7kslLS2pFwANxOGyQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTR1CmbEc8JJ52UW/unzS8l172tuz1ZX/rsB5L1afd359YObdmaXPd4xJ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Iwd2/axt5jbX6RXd607aG2IdOmJOubPjk+WV9yzT11b3vG8MYe5jHtBzfl1n7n3jeT6/Zu3Fx2O02xwju123fZQLVCz7aZbZW0R1KvpEPunj4KAkBlyvjT+ofunv4zCaByvGcHgigadpf0nJmtNLOOgR5gZh1m1mVmXQe1v+DmANSr6Mv4We6+w8zGSVpuZq+5+wv9H+DuCyUtlPo+oCu4PQB1KrRnd/cd2W2PpCclzSyjKQDlqzvsZjbKzE55576kKyWtK6sxAOUq8jJ+vKQnzeydn/Owuy8rpSuUptY4+rlLtibrz5z+gxpbqO6SCF/f9dvJ+lnLevOLPW+V3E3rq/t/yt23SLqgxF4ANBBDb0AQhB0IgrADQRB2IAjCDgTBpaSPA0PPOTu3Nv3R/0iuu2D8ykLbft/j+aeRStLhUfnDX5vn/mOhbf9V28Zk/Zlbz8+tHdw7ObnuCT/9n3paamns2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZjwE9N12arF9zY2du7QtjNhTa9u7Dv0rWx64a8KrFvzb64X/JrV00/+bkun//N4uS9StO3Jes//P5+afnfv++05PrPjbrvGS9961dyXorYs8OBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EwZXMLGDJ+XLL+gefeSNb/buza3FrHttnJdVc89XvJ+sg3078fY+7/ebJeyMXp3j774KPJeq1x+JS5r30kWT/hw+lx9sO//GXd2y4iNWUze3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIJx9iYYOiF97nT7sm3JemocvZYPX/axZL1305a6f3blZuZfF16SPvPw47m1OScWGwef/kD6XPzJX2rg8QcJhcbZzWyRmfWY2bp+y9rMbLmZbcpuR5fZMIDyDeZl/HclzTli2e2SOt19mqTO7HsALaxm2N39BUlHHhs4T9Li7P5iSVeV3BeAktX7Ad14d++WpOw29+BuM+swsy4z6zqo/XVuDkBRDf803t0Xunu7u7cP04hGbw5AjnrDvtPMJkhSdttTXksAGqHesD8taX52f76kH5XTDoBGqXndeDN7RNJlksaa2XZJX5G0QNJjZnaDpNclXdPIJo912/9sSrL+9NilyfqtOy5J1l9eeGFubczml5PrHtN+kT7+4Ju/e0Fu7VvLxiTXfeZ9P66rpVZWM+zufl1OKd7RMcAxjMNlgSAIOxAEYQeCIOxAEIQdCIIpm0uw79lzkvXl0+9K1vd6+m/uinvzh9YkacwD1ZxO2ep8f/7h2fu+NjG98ndKbqYFsGcHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZx+kHZ+/NLf2k3PvTK7bW+Nq3fM25J1Y2KeNcfTSDX+72CXSfGo1UzIXwZ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnD2z59qLk/WffDp/LH3ikJOS63Zsm52sj7hya7KO8u2ddGKh9TfMfiBZn6v0NQiqwJ4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4IIM84+ZExbsj750xuT9dRY+pd7ZiTX7Z5/erIuba5RRz2Gnj0ptzbptk2FfvaFr3w8WT9dGwr9/EaouWc3s0Vm1mNm6/otu8PM3jCz1dnX3Ma2CaCowbyM/66kOQMsv9vdZ2RfS8ttC0DZaobd3V+QtKsJvQBooCIf0N1iZmuyl/mj8x5kZh1m1mVmXQdV7LpfAOpXb9i/LWmqpBmSuiV9I++B7r7Q3dvdvX2YRtS5OQBF1RV2d9/p7r3ufljSfZJmltsWgLLVFXYzm9Dv26slrct7LIDWUHOc3cwekXSZpLFmtl3SVyRdZmYzJLmkrZI+1cAeSzF5Wfo6398846Vk/ZB6c2svfjl9LvzIjb9I1lEfG5r+9T3vqW25ta+OW5Vc9/VD6d+X0+4pdj58FWqG3d0HmsHgOJyqHji+cbgsEARhB4Ig7EAQhB0IgrADQYQ5xfXeiS8n67WmVf7Qa3+SWxv5Y4bWGuHQH/1+sv72Z/ck618dt6TubX/01RuS9dOeTw/dtSL27EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRJhx9qK2dp2ZW5ui/FMpo0udhrrt8+lrnjx5413J+tSh9Z9mev7P/yJZP+vj6UtN1zgsoyWxZweCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIBhnH6TzL8mfVnn/mROT6x7a/kbZ7ZRm6OSzkvWdf5z+t4382M5k/TNTOnNrHx1V6zoAxS7XfO5PP5Fbe2/H1uS6vfuPv6nK2LMDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMsw/S41Ofza1d/dDc5Lo9iy9J1mffvKKunsow7cSVyfonT32qSZ28201vzErW19x9QbL+3mUbc2u9u3fX1dOxrOae3cwmmdnzZrbBzNab2a3Z8jYzW25mm7Lb0Y1vF0C9BvMy/pCkz7n7uZIulnSzmU2XdLukTnefJqkz+x5Ai6oZdnfvdvdV2f09kjZImihpnqTF2cMWS7qqUU0CKO6oPqAzs8mS3i9phaTx7t4t9f1BkDQuZ50OM+sys66DOv6ONwaOFYMOu5mdLOmHkm5z90F/uuHuC9293d3bh2lEPT0CKMGgwm5mw9QX9Ifc/Yls8U4zm5DVJ0jqaUyLAMpg7umL4pqZqe89+S53v63f8rskveXuC8zsdklt7v7XqZ/1Hmvzi+zyEto+ev/75xcn6zd+6Ylk/fpT/qvMdo4by/elT0N9dV/+KbRPfi39u/Bbj6anRfaDB5L1iFZ4p3b7LhuoNphx9lmSrpe01sxWZ8u+KGmBpMfM7AZJr0u6poxmATRGzbC7+88kDfiXQlI1u2kAR43DZYEgCDsQBGEHgiDsQBCEHQgizCmupz74crK+ZP0VyfrbD76YW/vLUzck1x2mIcn6mgPpeiN97630aaQvPnxhsn5G565k/fCa13Jrpyr9f3IsTovcytizA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQNc9nL1OV57M3Us9Nlybrh05Or3/GnS+V2A0iS53Pzp4dCIKwA0EQdiAIwg4EQdiBIAg7EARhB4IIcz57I427l3FytD727EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQRM2wm9kkM3vezDaY2XozuzVbfoeZvWFmq7OvuY1vF0C9BnNQzSFJn3P3VWZ2iqSVZrY8q93t7l9vXHsAyjKY+dm7JXVn9/eY2QZJExvdGIByHdV7djObLOn9klZki24xszVmtsjMRues02FmXWbWdVD7CzULoH6DDruZnSzph5Juc/fdkr4taaqkGerb839joPXcfaG7t7t7+zCNKKFlAPUYVNjNbJj6gv6Quz8hSe6+09173f2wpPskzWxcmwCKGsyn8SbpO5I2uPs/9Fs+od/Drpa0rvz2AJRlMJ/Gz5J0vaS1ZrY6W/ZFSdeZ2Qz1zay7VdKnGtIhgFIM5tP4n0ka6DrUS8tvB0CjcAQdEARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCHP35m3M7L8l/We/RWMlvdm0Bo5Oq/bWqn1J9FavMns7291PG6jQ1LC/a+NmXe7eXlkDCa3aW6v2JdFbvZrVGy/jgSAIOxBE1WFfWPH2U1q1t1btS6K3ejWlt0rfswNonqr37ACahLADQVQSdjObY2YbzWyzmd1eRQ95zGyrma3NpqHuqriXRWbWY2br+i1rM7PlZrYpux1wjr2KemuJabwT04xX+txVPf1509+zm9kQSf8m6QpJ2yW9Iuk6d//XpjaSw8y2Smp398oPwDCz2ZL2Svqeu5+XLbtT0i53X5D9oRzt7l9okd7ukLS36mm8s9mKJvSfZlzSVZI+oQqfu0Rff6omPG9V7NlnStrs7lvc/YCkJZLmVdBHy3P3FyTtOmLxPEmLs/uL1ffL0nQ5vbUEd+9291XZ/T2S3plmvNLnLtFXU1QR9omStvX7frtaa753l/Scma00s46qmxnAeHfvlvp+eSSNq7ifI9WcxruZjphmvGWeu3qmPy+qirAPNJVUK43/zXL3CyV9UNLN2ctVDM6gpvFulgGmGW8J9U5/XlQVYd8uaVK/78+UtKOCPgbk7juy2x5JT6r1pqLe+c4MutltT8X9/ForTeM90DTjaoHnrsrpz6sI+yuSppnZOWY2XNK1kp6uoI93MbNR2QcnMrNRkq5U601F/bSk+dn9+ZJ+VGEvv6FVpvHOm2ZcFT93lU9/7u5N/5I0V32fyP+7pL+tooecvqZIejX7Wl91b5IeUd/LuoPqe0V0g6Qxkjolbcpu21qot+9LWitpjfqCNaGi3v5AfW8N10hanX3Nrfq5S/TVlOeNw2WBIDiCDgiCsANBEHYgCMIOBEHYgSAIOxAEYQeC+H/0h3rgdD8QegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3041, grad_fn=<NllLossBackward>), tensor(0.9219))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model , opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())\n",
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs, sampler=RandomSampler(train_ds), collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, bs, sampler=SequentialSampler(valid_ds), collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4102, grad_fn=<NllLossBackward>), tensor(0.8438))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model , opt = get_model()\n",
    "fit()\n",
    "\n",
    "loss,acc = loss_func(model(xb), yb.long()), accuracy(model(xb), yb.long())\n",
    "assert acc>0.7\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb.long())\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                tot_loss += loss_func(pred, yb.long())\n",
    "                tot_acc += accuracy(pred, yb.long())\n",
    "                \n",
    "        nv = len(valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv,tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.0938) tensor(0.8283)\n",
      "1 tensor(0.4576) tensor(0.8949)\n",
      "2 tensor(0.4269) tensor(0.8730)\n",
      "3 tensor(0.2561) tensor(0.9355)\n",
      "4 tensor(0.3563) tensor(0.9206)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3563), tensor(0.9206))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return(DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "           DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.3760) tensor(0.9076)\n",
      "1 tensor(0.6093) tensor(0.8739)\n",
      "2 tensor(0.2139) tensor(0.9423)\n",
      "3 tensor(0.2168) tensor(0.9422)\n",
      "4 tensor(0.2220) tensor(0.9399)\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_dls(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "loss, acc = fit(5, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBunch and Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "nh,bs = 50,64\n",
    "c = int(y_train.max().item()+1)\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBunch():\n",
    "    def __init__(self, train_dl, valid_dl, c=None):\n",
    "        self.train_dl, self.valid_dl, self.c = train_dl, valid_dl, c\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "    \n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(data, lr=0.5, nh=50):\n",
    "    m = data.train_ds.x.shape[1]\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c))\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, opt, loss_func, data):\n",
    "        self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, learn):\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        for xb,yb in learn.data.train_dl:\n",
    "            pred = learn.model(xb)\n",
    "            loss = learn.loss_func(pred, yb.long())\n",
    "\n",
    "            loss.backward()\n",
    "            learn.opt.step()\n",
    "            learn.opt.zero_grad()\n",
    "            \n",
    "        learn.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tot_loss, tot_acc = 0., 0.\n",
    "            for xb, yb in learn.data.valid_dl:\n",
    "                pred = learn.model(xb)\n",
    "                tot_loss += learn.loss_func(pred, yb.long())\n",
    "                tot_acc += accuracy(pred, yb.long())\n",
    "                \n",
    "        nv = len(learn.data.valid_dl)\n",
    "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
    "    return tot_loss/nv,tot_acc/nv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.1285) tensor(0.7803)\n"
     ]
    }
   ],
   "source": [
    "loss, acc = fit(1, learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
    "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
    "def camel2snake(name):\n",
    "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
    "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
    "\n",
    "class Callback():\n",
    "    _order = 0\n",
    "    def set_runner(self, run): self.run = run\n",
    "    def __getattr__(self, k): return getattr(self.run, k)\n",
    "    @property\n",
    "    def name(self):\n",
    "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
    "        return camel2snake(name or 'callback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs=0\n",
    "        self.run.n_iter=0\n",
    "        \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter += 1\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs = self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train = True\n",
    "        \n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(Callback):\n",
    "    def after_step(self):\n",
    "        if self.train_eval.n_iters>=10: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval_callback'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbname = 'TrainEvalCallback'\n",
    "camel2snake(cbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_eval'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainEvalCallback().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "def listify(o):\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, cbs=None, cb_funcs=None):\n",
    "        cbs = listify(cbs)\n",
    "        for cbf in listify(cb_funcs):\n",
    "            cb = cbf()\n",
    "            setattr(self, cb.name, cb)\n",
    "            cbs.append(cb)\n",
    "        self.stop, self.cbs = False, [TrainEvalCallback()]+cbs\n",
    "    \n",
    "    @property\n",
    "    def opt(self): return self.learn.opt\n",
    "    @property\n",
    "    def model(self): return self.learn.model\n",
    "    @property\n",
    "    def loss_func(self): return self.learn.loss_func\n",
    "    @property\n",
    "    def data(self): return self.learn.data\n",
    "    \n",
    "    def one_batch(self, xb, yb):\n",
    "        self.xb, self.yb = xb, yb\n",
    "        if self('begin_batch'): return\n",
    "        self.pred = self.model(self.xb)\n",
    "        if self('after_pred'): return\n",
    "        self.loss = self.loss_func(self.pred, self.yb.long())\n",
    "        if self('after_loss') or not self.in_train: return\n",
    "        self.loss.backward()\n",
    "        if self('after_backward'): return\n",
    "        self.opt.step()\n",
    "        if self('after_step'): return\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "    def all_batches(self, dl):\n",
    "        self.iters = len(dl)\n",
    "        for xb, yb in dl:\n",
    "            if self.stop: break\n",
    "            self.one_batch(xb, yb)\n",
    "            self('after_batch')\n",
    "        self.stop = False\n",
    "        \n",
    "    def fit(self, epochs, learn):\n",
    "        self.epochs, self.learn = epochs, learn\n",
    "        \n",
    "        try: \n",
    "            for cb in self.cbs: cb.set_runner(self)\n",
    "            if self('begin_fit'): return\n",
    "            for epoch in range(epochs):\n",
    "                self.epoch = epoch\n",
    "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
    "                if self('after_epoch'): break\n",
    "        \n",
    "        finally:\n",
    "            self('after_fit')\n",
    "            self.learn = None\n",
    "            \n",
    "    def __call__(self, cb_name):\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, cb_name, None)\n",
    "            if f and f(): return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): self.metrics, self.in_train = listify(metrics), in_train\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tot_loss, self.count = 0., 0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "        \n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    \n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if not self.count: return ''\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "        \n",
    "    def accumulate(self, run):\n",
    "        bn = run.xb.shape[0]\n",
    "        self.tot_loss += run.loss * bn\n",
    "        self.count += bn\n",
    "        \n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
    "    \n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats, self.valid_stats = AvgStats(metrics, True), AvgStats(metrics, False)\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        \n",
    "    def after_loss(self):\n",
    "        stats = self.train_stats if self.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(self.run)\n",
    "            \n",
    "    def after_epoch(self):\n",
    "        print(self.train_stats)\n",
    "        print(self.valid_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(*get_model(data), loss_func, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = AvgStatsCallback([accuracy])\n",
    "run = Runner(cbs=stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.5143828125, tensor(0.8507)]\n",
      "valid: [0.32987255859375, tensor(0.9076)]\n",
      "train: [0.26448880859375, tensor(0.9267)]\n",
      "valid: [0.26346669921875, tensor(0.9301)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(2, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26346669921875, tensor(0.9301))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,acc = stats.valid_stats.avg_stats\n",
    "assert acc>0.9\n",
    "loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cbf = partial(AvgStatsCallback, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Runner(cb_funcs=acc_cbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [0.22152916015625, tensor(0.9382)]\n",
      "valid: [0.1876039306640625, tensor(0.9479)]\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1876039306640625, tensor(0.9479)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.avg_stats.valid_stats.avg_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
